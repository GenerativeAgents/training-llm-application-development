{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain の記法解説 (LCEL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T03:27:59.846978Z",
     "iopub.status.busy": "2025-02-16T03:27:59.846480Z",
     "iopub.status.idle": "2025-02-16T03:27:59.863693Z",
     "shell.execute_reply": "2025-02-16T03:27:59.863251Z"
    }
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(dotenv_path=\"../.env\", override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runnable と RunnableSequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T03:27:59.865773Z",
     "iopub.status.busy": "2025-02-16T03:27:59.865587Z",
     "iopub.status.idle": "2025-02-16T03:28:00.464272Z",
     "shell.execute_reply": "2025-02-16T03:28:00.464048Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"ユーザーが入力した料理のレシピを考えてください。\"),\n",
    "        (\"human\", \"{dish}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4.1-nano\", temperature=0)\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T03:28:00.465544Z",
     "iopub.status.busy": "2025-02-16T03:28:00.465451Z",
     "iopub.status.idle": "2025-02-16T03:28:15.233256Z",
     "shell.execute_reply": "2025-02-16T03:28:15.232803Z"
    }
   },
   "outputs": [],
   "source": [
    "prompt_value = prompt.invoke({\"dish\": \"カレー\"})\n",
    "ai_message = model.invoke(prompt_value)\n",
    "output = output_parser.invoke(ai_message)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T03:28:15.239900Z",
     "iopub.status.busy": "2025-02-16T03:28:15.239230Z",
     "iopub.status.idle": "2025-02-16T03:28:38.276536Z",
     "shell.execute_reply": "2025-02-16T03:28:38.273534Z"
    }
   },
   "outputs": [],
   "source": [
    "chain = prompt | model | output_parser\n",
    "\n",
    "output = chain.invoke({\"dish\": \"カレー\"})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runnable の実行方法―invoke・stream・batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T03:28:38.296978Z",
     "iopub.status.busy": "2025-02-16T03:28:38.296808Z",
     "iopub.status.idle": "2025-02-16T03:28:48.031407Z",
     "shell.execute_reply": "2025-02-16T03:28:48.030821Z"
    }
   },
   "outputs": [],
   "source": [
    "chain = prompt | model | output_parser\n",
    "\n",
    "for chunk in chain.stream({\"dish\": \"カレー\"}):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T03:28:48.033984Z",
     "iopub.status.busy": "2025-02-16T03:28:48.033699Z",
     "iopub.status.idle": "2025-02-16T03:28:58.797188Z",
     "shell.execute_reply": "2025-02-16T03:28:58.796536Z"
    }
   },
   "outputs": [],
   "source": [
    "chain = prompt | model | output_parser\n",
    "\n",
    "inputs = [{\"dish\": \"カレー\"}, {\"dish\": \"うどん\"}]\n",
    "outputs = chain.batch(inputs)\n",
    "\n",
    "for i, o in zip(inputs, outputs):\n",
    "    print(f\"input: {i}\")\n",
    "    print(f\"output: {o[:30]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LCEL の「|」で様々な Runnable を連鎖させる\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T03:28:58.800303Z",
     "iopub.status.busy": "2025-02-16T03:28:58.800132Z",
     "iopub.status.idle": "2025-02-16T03:28:58.817300Z",
     "shell.execute_reply": "2025-02-16T03:28:58.817001Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4.1-nano\", temperature=0)\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T03:28:58.818615Z",
     "iopub.status.busy": "2025-02-16T03:28:58.818492Z",
     "iopub.status.idle": "2025-02-16T03:28:58.820683Z",
     "shell.execute_reply": "2025-02-16T03:28:58.820431Z"
    }
   },
   "outputs": [],
   "source": [
    "cot_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"ユーザーの質問にステップバイステップで回答してください。\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "cot_chain = cot_prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T03:28:58.821983Z",
     "iopub.status.busy": "2025-02-16T03:28:58.821880Z",
     "iopub.status.idle": "2025-02-16T03:28:58.823787Z",
     "shell.execute_reply": "2025-02-16T03:28:58.823539Z"
    }
   },
   "outputs": [],
   "source": [
    "summarize_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"ステップバイステップで考えた回答から結論だけ抽出してください。\"),\n",
    "        (\"human\", \"{text}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "summarize_chain = summarize_prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T03:28:58.825077Z",
     "iopub.status.busy": "2025-02-16T03:28:58.824990Z",
     "iopub.status.idle": "2025-02-16T03:29:01.350887Z",
     "shell.execute_reply": "2025-02-16T03:29:01.350196Z"
    }
   },
   "outputs": [],
   "source": [
    "cot_summarize_chain = cot_chain | summarize_chain\n",
    "output = cot_summarize_chain.invoke({\"question\": \"10 + 2 * 3\"})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T03:29:01.353900Z",
     "iopub.status.busy": "2025-02-16T03:29:01.353618Z",
     "iopub.status.idle": "2025-02-16T03:29:02.755043Z",
     "shell.execute_reply": "2025-02-16T03:29:02.754353Z"
    }
   },
   "outputs": [],
   "source": [
    "# このセルのコードでは、LCELで記述したチェーンを可視化します。\n",
    "# mermaid.inkのサービスを利用しているため以下のエラーが発生する場合がありますが、エラーになった場合でも続きのハンズオンには影響はありません。\n",
    "# ReadTimeout: HTTPSConnectionPool(host='mermaid.ink', port=443): Read timed out. (read timeout=10)\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(cot_summarize_chain.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 補足）ステップバイステップの回答と要約を一度に出力する例\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "class Answer(BaseModel):\n",
    "    # Zero-shot CoTの効果を期待して、reasoningをfinal_answerより先に出力\n",
    "    reasoning: str = Field(description=\"ステップバイステップの回答\")\n",
    "    final_answer: str = Field(description=\"最終的な回答\")\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"ユーザーの質問に回答してください。\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4.1-nano\", temperature=0)\n",
    "\n",
    "chain = prompt | model.with_structured_output(Answer)\n",
    "\n",
    "output = chain.invoke({\"question\": \"10 + 2 * 3 - 4 * 18 / 9\"})\n",
    "print(output.model_dump_json(indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RunnableParallel―複数の Runnable を並列で処理する\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T03:29:04.781555Z",
     "iopub.status.busy": "2025-02-16T03:29:04.781188Z",
     "iopub.status.idle": "2025-02-16T03:29:04.800886Z",
     "shell.execute_reply": "2025-02-16T03:29:04.800521Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4.1-nano\", temperature=0)\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T03:29:04.802661Z",
     "iopub.status.busy": "2025-02-16T03:29:04.802533Z",
     "iopub.status.idle": "2025-02-16T03:29:04.805072Z",
     "shell.execute_reply": "2025-02-16T03:29:04.804738Z"
    }
   },
   "outputs": [],
   "source": [
    "optimistic_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"あなたは楽観主義者です。ユーザーの入力に対して楽観的な意見をください。\",\n",
    "        ),\n",
    "        (\"human\", \"{topic}\"),\n",
    "    ]\n",
    ")\n",
    "optimistic_chain = optimistic_prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T03:29:04.806531Z",
     "iopub.status.busy": "2025-02-16T03:29:04.806391Z",
     "iopub.status.idle": "2025-02-16T03:29:04.808718Z",
     "shell.execute_reply": "2025-02-16T03:29:04.808424Z"
    }
   },
   "outputs": [],
   "source": [
    "pessimistic_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"あなたは悲観主義者です。ユーザーの入力に対して悲観的な意見をください。\",\n",
    "        ),\n",
    "        (\"human\", \"{topic}\"),\n",
    "    ]\n",
    ")\n",
    "pessimistic_chain = pessimistic_prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T03:29:04.810085Z",
     "iopub.status.busy": "2025-02-16T03:29:04.809960Z",
     "iopub.status.idle": "2025-02-16T03:29:13.389316Z",
     "shell.execute_reply": "2025-02-16T03:29:13.388763Z"
    }
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "parallel_chain = RunnableParallel(\n",
    "    {\n",
    "        \"optimistic_opinion\": optimistic_chain,\n",
    "        \"pessimistic_opinion\": pessimistic_chain,\n",
    "    }\n",
    ")\n",
    "\n",
    "output = parallel_chain.invoke({\"topic\": \"生成AIの進化について\"})\n",
    "pprint.pprint(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T03:29:13.392091Z",
     "iopub.status.busy": "2025-02-16T03:29:13.391834Z",
     "iopub.status.idle": "2025-02-16T03:29:14.780205Z",
     "shell.execute_reply": "2025-02-16T03:29:14.779568Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(parallel_chain.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RunnableParallel の出力を Runnable の入力に連結する\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T03:29:14.783199Z",
     "iopub.status.busy": "2025-02-16T03:29:14.782848Z",
     "iopub.status.idle": "2025-02-16T03:29:14.786411Z",
     "shell.execute_reply": "2025-02-16T03:29:14.785935Z"
    }
   },
   "outputs": [],
   "source": [
    "synthesize_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"あなたは客観的AIです。2つの意見をまとめてください。\"),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"楽観的意見: {optimistic_opinion}\\n悲観的意見: {pessimistic_opinion}\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "synthesize_chain = synthesize_prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T03:29:14.788560Z",
     "iopub.status.busy": "2025-02-16T03:29:14.788386Z",
     "iopub.status.idle": "2025-02-16T03:29:22.283814Z",
     "shell.execute_reply": "2025-02-16T03:29:22.283251Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "synthesize_chain = (\n",
    "    RunnableParallel(\n",
    "        {\n",
    "            \"optimistic_opinion\": optimistic_chain,\n",
    "            \"pessimistic_opinion\": pessimistic_chain,\n",
    "        }\n",
    "    )\n",
    "    | synthesize_chain\n",
    ")\n",
    "\n",
    "output = synthesize_chain.invoke({\"topic\": \"生成AIの進化について\"})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T03:29:22.286864Z",
     "iopub.status.busy": "2025-02-16T03:29:22.286573Z",
     "iopub.status.idle": "2025-02-16T03:29:23.680683Z",
     "shell.execute_reply": "2025-02-16T03:29:23.680164Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(synthesize_chain.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
