{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain の記法解説 (LCEL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T03:27:59.846978Z",
     "iopub.status.busy": "2025-02-16T03:27:59.846480Z",
     "iopub.status.idle": "2025-02-16T03:27:59.863693Z",
     "shell.execute_reply": "2025-02-16T03:27:59.863251Z"
    }
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(dotenv_path=\"../.env\", override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runnable と RunnableSequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T03:27:59.865773Z",
     "iopub.status.busy": "2025-02-16T03:27:59.865587Z",
     "iopub.status.idle": "2025-02-16T03:28:00.464272Z",
     "shell.execute_reply": "2025-02-16T03:28:00.464048Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"ユーザーが入力した料理のレシピを考えてください。\"),\n",
    "        (\"human\", \"{dish}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4.1-nano\", temperature=0)\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T03:28:00.465544Z",
     "iopub.status.busy": "2025-02-16T03:28:00.465451Z",
     "iopub.status.idle": "2025-02-16T03:28:15.233256Z",
     "shell.execute_reply": "2025-02-16T03:28:15.232803Z"
    }
   },
   "outputs": [],
   "source": [
    "prompt_value = prompt.invoke({\"dish\": \"カレー\"})\n",
    "ai_message = model.invoke(prompt_value)\n",
    "output = output_parser.invoke(ai_message)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T03:28:15.239900Z",
     "iopub.status.busy": "2025-02-16T03:28:15.239230Z",
     "iopub.status.idle": "2025-02-16T03:28:38.276536Z",
     "shell.execute_reply": "2025-02-16T03:28:38.273534Z"
    }
   },
   "outputs": [],
   "source": [
    "chain = prompt | model | output_parser\n",
    "\n",
    "output = chain.invoke({\"dish\": \"カレー\"})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runnable の実行方法―invoke・stream・batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T03:28:38.296978Z",
     "iopub.status.busy": "2025-02-16T03:28:38.296808Z",
     "iopub.status.idle": "2025-02-16T03:28:48.031407Z",
     "shell.execute_reply": "2025-02-16T03:28:48.030821Z"
    }
   },
   "outputs": [],
   "source": [
    "chain = prompt | model | output_parser\n",
    "\n",
    "for chunk in chain.stream({\"dish\": \"カレー\"}):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T03:28:48.033984Z",
     "iopub.status.busy": "2025-02-16T03:28:48.033699Z",
     "iopub.status.idle": "2025-02-16T03:28:58.797188Z",
     "shell.execute_reply": "2025-02-16T03:28:58.796536Z"
    }
   },
   "outputs": [],
   "source": [
    "chain = prompt | model | output_parser\n",
    "\n",
    "inputs = [{\"dish\": \"カレー\"}, {\"dish\": \"うどん\"}]\n",
    "outputs = chain.batch(inputs)\n",
    "\n",
    "for i, o in zip(inputs, outputs):\n",
    "    print(f\"input: {i}\")\n",
    "    print(f\"output: {o[:30]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LCEL の「|」で様々な Runnable を連鎖させる\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T03:28:58.800303Z",
     "iopub.status.busy": "2025-02-16T03:28:58.800132Z",
     "iopub.status.idle": "2025-02-16T03:28:58.817300Z",
     "shell.execute_reply": "2025-02-16T03:28:58.817001Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4.1-nano\", temperature=0)\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T03:28:58.818615Z",
     "iopub.status.busy": "2025-02-16T03:28:58.818492Z",
     "iopub.status.idle": "2025-02-16T03:28:58.820683Z",
     "shell.execute_reply": "2025-02-16T03:28:58.820431Z"
    }
   },
   "outputs": [],
   "source": [
    "cot_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"ユーザーの質問にステップバイステップで回答してください。\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "cot_chain = cot_prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T03:28:58.821983Z",
     "iopub.status.busy": "2025-02-16T03:28:58.821880Z",
     "iopub.status.idle": "2025-02-16T03:28:58.823787Z",
     "shell.execute_reply": "2025-02-16T03:28:58.823539Z"
    }
   },
   "outputs": [],
   "source": [
    "summarize_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"ステップバイステップで考えた回答から結論だけ抽出してください。\"),\n",
    "        (\"human\", \"{text}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "summarize_chain = summarize_prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T03:28:58.825077Z",
     "iopub.status.busy": "2025-02-16T03:28:58.824990Z",
     "iopub.status.idle": "2025-02-16T03:29:01.350887Z",
     "shell.execute_reply": "2025-02-16T03:29:01.350196Z"
    }
   },
   "outputs": [],
   "source": [
    "cot_summarize_chain = cot_chain | summarize_chain\n",
    "output = cot_summarize_chain.invoke({\"question\": \"10 + 2 * 3\"})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T03:29:01.353900Z",
     "iopub.status.busy": "2025-02-16T03:29:01.353618Z",
     "iopub.status.idle": "2025-02-16T03:29:02.755043Z",
     "shell.execute_reply": "2025-02-16T03:29:02.754353Z"
    }
   },
   "outputs": [],
   "source": [
    "# このセルのコードでは、LCELで記述したチェーンを可視化します。\n",
    "# mermaid.inkのサービスを利用しているため以下のエラーが発生する場合がありますが、エラーになった場合でも続きのハンズオンには影響はありません。\n",
    "# ReadTimeout: HTTPSConnectionPool(host='mermaid.ink', port=443): Read timed out. (read timeout=10)\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(cot_summarize_chain.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RunnableLambda―任意の関数を Runnable にする\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T03:29:02.758284Z",
     "iopub.status.busy": "2025-02-16T03:29:02.757997Z",
     "iopub.status.idle": "2025-02-16T03:29:02.778691Z",
     "shell.execute_reply": "2025-02-16T03:29:02.778239Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4.1-nano\", temperature=0)\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T03:29:02.780925Z",
     "iopub.status.busy": "2025-02-16T03:29:02.780716Z",
     "iopub.status.idle": "2025-02-16T03:29:03.231393Z",
     "shell.execute_reply": "2025-02-16T03:29:03.230816Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "\n",
    "def upper(text: str) -> str:\n",
    "    return text.upper()\n",
    "\n",
    "\n",
    "chain = prompt | model | output_parser | RunnableLambda(upper)\n",
    "\n",
    "output = chain.invoke({\"input\": \"Hello!\"})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RunnableLambda への自動変換\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T03:29:03.795175Z",
     "iopub.status.busy": "2025-02-16T03:29:03.794896Z",
     "iopub.status.idle": "2025-02-16T03:29:03.798667Z",
     "shell.execute_reply": "2025-02-16T03:29:03.798250Z"
    }
   },
   "outputs": [],
   "source": [
    "def upper(text: str) -> str:\n",
    "    return text.upper()\n",
    "\n",
    "\n",
    "chain = prompt | model | output_parser | upper\n",
    "output = chain.invoke({\"input\": \"Hello!\"})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runnable の入力の型と出力の型に注意\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T03:29:04.288227Z",
     "iopub.status.busy": "2025-02-16T03:29:04.287995Z",
     "iopub.status.idle": "2025-02-16T03:29:04.291463Z",
     "shell.execute_reply": "2025-02-16T03:29:04.290909Z"
    }
   },
   "outputs": [],
   "source": [
    "def upper(text: str) -> str:\n",
    "    return text.upper()\n",
    "\n",
    "\n",
    "chain = prompt | model | upper\n",
    "\n",
    "# 以下のコードを実行するとエラーになります\n",
    "# output = chain.invoke({\"input\": \"Hello!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T03:29:04.293505Z",
     "iopub.status.busy": "2025-02-16T03:29:04.293347Z",
     "iopub.status.idle": "2025-02-16T03:29:04.296049Z",
     "shell.execute_reply": "2025-02-16T03:29:04.295646Z"
    }
   },
   "outputs": [],
   "source": [
    "chain = prompt | model | StrOutputParser() | upper\n",
    "\n",
    "output = chain.invoke({\"input\": \"Hello!\"})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RunnableParallel―複数の Runnable を並列で処理する\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T03:29:04.781555Z",
     "iopub.status.busy": "2025-02-16T03:29:04.781188Z",
     "iopub.status.idle": "2025-02-16T03:29:04.800886Z",
     "shell.execute_reply": "2025-02-16T03:29:04.800521Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4.1-nano\", temperature=0)\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T03:29:04.802661Z",
     "iopub.status.busy": "2025-02-16T03:29:04.802533Z",
     "iopub.status.idle": "2025-02-16T03:29:04.805072Z",
     "shell.execute_reply": "2025-02-16T03:29:04.804738Z"
    }
   },
   "outputs": [],
   "source": [
    "optimistic_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"あなたは楽観主義者です。ユーザーの入力に対して楽観的な意見をください。\",\n",
    "        ),\n",
    "        (\"human\", \"{topic}\"),\n",
    "    ]\n",
    ")\n",
    "optimistic_chain = optimistic_prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T03:29:04.806531Z",
     "iopub.status.busy": "2025-02-16T03:29:04.806391Z",
     "iopub.status.idle": "2025-02-16T03:29:04.808718Z",
     "shell.execute_reply": "2025-02-16T03:29:04.808424Z"
    }
   },
   "outputs": [],
   "source": [
    "pessimistic_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"あなたは悲観主義者です。ユーザーの入力に対して悲観的な意見をください。\",\n",
    "        ),\n",
    "        (\"human\", \"{topic}\"),\n",
    "    ]\n",
    ")\n",
    "pessimistic_chain = pessimistic_prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T03:29:04.810085Z",
     "iopub.status.busy": "2025-02-16T03:29:04.809960Z",
     "iopub.status.idle": "2025-02-16T03:29:13.389316Z",
     "shell.execute_reply": "2025-02-16T03:29:13.388763Z"
    }
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "parallel_chain = RunnableParallel(\n",
    "    {\n",
    "        \"optimistic_opinion\": optimistic_chain,\n",
    "        \"pessimistic_opinion\": pessimistic_chain,\n",
    "    }\n",
    ")\n",
    "\n",
    "output = parallel_chain.invoke({\"topic\": \"生成AIの進化について\"})\n",
    "pprint.pprint(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T03:29:13.392091Z",
     "iopub.status.busy": "2025-02-16T03:29:13.391834Z",
     "iopub.status.idle": "2025-02-16T03:29:14.780205Z",
     "shell.execute_reply": "2025-02-16T03:29:14.779568Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(parallel_chain.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RunnableParallel の出力を Runnable の入力に連結する\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T03:29:14.783199Z",
     "iopub.status.busy": "2025-02-16T03:29:14.782848Z",
     "iopub.status.idle": "2025-02-16T03:29:14.786411Z",
     "shell.execute_reply": "2025-02-16T03:29:14.785935Z"
    }
   },
   "outputs": [],
   "source": [
    "synthesize_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"あなたは客観的AIです。2つの意見をまとめてください。\"),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"楽観的意見: {optimistic_opinion}\\n悲観的意見: {pessimistic_opinion}\",\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T03:29:14.788560Z",
     "iopub.status.busy": "2025-02-16T03:29:14.788386Z",
     "iopub.status.idle": "2025-02-16T03:29:22.283814Z",
     "shell.execute_reply": "2025-02-16T03:29:22.283251Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "synthesize_chain = (\n",
    "    RunnableParallel(\n",
    "        {\n",
    "            \"optimistic_opinion\": optimistic_chain,\n",
    "            \"pessimistic_opinion\": pessimistic_chain,\n",
    "        }\n",
    "    )\n",
    "    | synthesize_prompt\n",
    "    | model\n",
    "    | output_parser\n",
    ")\n",
    "\n",
    "output = synthesize_chain.invoke({\"topic\": \"生成AIの進化について\"})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T03:29:22.286864Z",
     "iopub.status.busy": "2025-02-16T03:29:22.286573Z",
     "iopub.status.idle": "2025-02-16T03:29:23.680683Z",
     "shell.execute_reply": "2025-02-16T03:29:23.680164Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(synthesize_chain.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RunnableParallel への自動変換\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T03:29:23.683487Z",
     "iopub.status.busy": "2025-02-16T03:29:23.683268Z",
     "iopub.status.idle": "2025-02-16T03:29:23.687465Z",
     "shell.execute_reply": "2025-02-16T03:29:23.686851Z"
    }
   },
   "outputs": [],
   "source": [
    "synthesize_chain = (\n",
    "    {\n",
    "        \"optimistic_opinion\": optimistic_chain,\n",
    "        \"pessimistic_opinion\": pessimistic_chain,\n",
    "    }\n",
    "    | synthesize_prompt\n",
    "    | model\n",
    "    | output_parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T03:29:23.689783Z",
     "iopub.status.busy": "2025-02-16T03:29:23.689582Z",
     "iopub.status.idle": "2025-02-16T03:29:31.759578Z",
     "shell.execute_reply": "2025-02-16T03:29:31.759312Z"
    }
   },
   "outputs": [],
   "source": [
    "output = synthesize_chain.invoke({\"topic\": \"生成AIの進化について\"})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RunnablePassthrough―入力をそのまま出力する\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T03:29:45.179696Z",
     "iopub.status.busy": "2025-02-16T03:29:45.179606Z",
     "iopub.status.idle": "2025-02-16T03:29:56.462542Z",
     "shell.execute_reply": "2025-02-16T03:29:56.462257Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "loader = DirectoryLoader(\n",
    "    path=\"../tmp/langchain\",\n",
    "    glob=\"**/*.mdx\",\n",
    "    loader_cls=TextLoader,\n",
    ")\n",
    "raw_docs = loader.load()\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(raw_docs)\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "db = Chroma.from_documents(docs, embeddings)\n",
    "\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T03:29:56.466611Z",
     "iopub.status.busy": "2025-02-16T03:29:56.466437Z",
     "iopub.status.idle": "2025-02-16T03:29:56.476301Z",
     "shell.execute_reply": "2025-02-16T03:29:56.476082Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template('''\\\n",
    "以下の文脈だけを踏まえて質問に回答してください。\n",
    "\n",
    "文脈: \"\"\"\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "質問: {question}\n",
    "''')\n",
    "\n",
    "model = ChatOpenAI(model_name=\"gpt-4.1-nano\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T03:29:56.477357Z",
     "iopub.status.busy": "2025-02-16T03:29:56.477274Z",
     "iopub.status.idle": "2025-02-16T03:29:59.293484Z",
     "shell.execute_reply": "2025-02-16T03:29:59.293203Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "output = chain.invoke(\"LangGraphとは\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### assign―RunnableParallel に値を追加する\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T03:29:59.294797Z",
     "iopub.status.busy": "2025-02-16T03:29:59.294714Z",
     "iopub.status.idle": "2025-02-16T03:30:01.660688Z",
     "shell.execute_reply": "2025-02-16T03:30:01.660295Z"
    }
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "chain = {\n",
    "    \"question\": RunnablePassthrough(),\n",
    "    \"context\": retriever,\n",
    "} | RunnablePassthrough.assign(answer=prompt | model | StrOutputParser())\n",
    "\n",
    "output = chain.invoke(\"LangGraphとは\")\n",
    "pprint.pprint(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LCEL を使わない実装\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_rag(query: str) -> str:\n",
    "    documents = retriever.invoke(query)\n",
    "    chain = prompt | model | StrOutputParser()\n",
    "    return chain.invoke({\"question\": query, \"context\": documents})\n",
    "\n",
    "\n",
    "output = invoke_rag(\"LangGraphとは\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import traceable\n",
    "\n",
    "\n",
    "@traceable\n",
    "def invoke_rag(query: str) -> str:\n",
    "    documents = retriever.invoke(query)\n",
    "    chain = prompt | model | StrOutputParser()\n",
    "    return chain.invoke({\"question\": query, \"context\": documents})\n",
    "\n",
    "\n",
    "output = invoke_rag(\"LangGraphとは\")\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
