{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain 入門\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T12:05:40.231766Z",
     "iopub.status.busy": "2025-03-04T12:05:40.231335Z",
     "iopub.status.idle": "2025-03-04T12:05:40.253963Z",
     "shell.execute_reply": "2025-03-04T12:05:40.253473Z"
    }
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(dotenv_path=\"../.env\", override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model の基本的な使い方\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T12:05:40.255572Z",
     "iopub.status.busy": "2025-03-04T12:05:40.255433Z",
     "iopub.status.idle": "2025-03-04T12:05:42.131083Z",
     "shell.execute_reply": "2025-03-04T12:05:42.130549Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\n",
    "    model=\"gpt-5-nano\",\n",
    "    model_provider=\"openai\",\n",
    "    reasoning_effort=\"minimal\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\"You are a helpful assistant.\"),\n",
    "    HumanMessage(\"こんにちは！私はジョンと言います\"),\n",
    "    AIMessage(\"こんにちは、ジョンさん！どのようにお手伝いできますか？\"),\n",
    "    HumanMessage(\"私の名前がわかりますか？\"),\n",
    "]\n",
    "\n",
    "ai_message = model.invoke(messages)\n",
    "print(ai_message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ストリーミング\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T12:05:42.132904Z",
     "iopub.status.busy": "2025-03-04T12:05:42.132766Z",
     "iopub.status.idle": "2025-03-04T12:05:42.958388Z",
     "shell.execute_reply": "2025-03-04T12:05:42.957648Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\n",
    "    model=\"gpt-5-nano\",\n",
    "    model_provider=\"openai\",\n",
    "    reasoning_effort=\"minimal\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\"You are a helpful assistant.\"),\n",
    "    HumanMessage(\"こんにちは！\"),\n",
    "]\n",
    "\n",
    "for chunk in model.stream(messages):\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt template\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatPromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T12:05:42.967693Z",
     "iopub.status.busy": "2025-03-04T12:05:42.967478Z",
     "iopub.status.idle": "2025-03-04T12:05:43.144957Z",
     "shell.execute_reply": "2025-03-04T12:05:43.144544Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"ユーザーが入力した料理のレシピを考えてください。\"),\n",
    "        (\"human\", \"{dish}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt_value = prompt.invoke({\"dish\": \"カレー\"})\n",
    "for message in prompt_value.messages:\n",
    "    print(f\"{message.type}: {message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## with_structured_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with_structured_output を使った構造化出力\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "\n",
    "class Recipe(BaseModel):\n",
    "    ingredients: list[str] = Field(description=\"ingredients of the dish\")\n",
    "    steps: list[str] = Field(description=\"steps to make the dish\")\n",
    "\n",
    "\n",
    "model = init_chat_model(\n",
    "    model=\"gpt-5-nano\",\n",
    "    model_provider=\"openai\",\n",
    "    reasoning_effort=\"minimal\",\n",
    ")\n",
    "structured_model = model.with_structured_output(Recipe)\n",
    "result = structured_model.invoke(\"カレーのレシピを教えて\")\n",
    "\n",
    "print(type(result))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参考）LCEL（LangChain Expression Language）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "\n",
    "class Recipe(BaseModel):\n",
    "    ingredients: list[str] = Field(description=\"ingredients of the dish\")\n",
    "    steps: list[str] = Field(description=\"steps to make the dish\")\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"ユーザーが入力した料理のレシピを考えてください。\"),\n",
    "        (\"human\", \"{dish}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = init_chat_model(\n",
    "    model=\"gpt-5-nano\",\n",
    "    model_provider=\"openai\",\n",
    "    reasoning_effort=\"minimal\",\n",
    ")\n",
    "\n",
    "chain = prompt | model.with_structured_output(Recipe)\n",
    "result = chain.invoke({\"dish\": \"カレー\"})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangSmith のセットアップとトレースの確認\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 実行する前に、Notebook上で「Restart」を実行してください\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(dotenv_path=\"../.env\", override=True)\n",
    "\n",
    "print(os.environ[\"LANGCHAIN_API_KEY\"][:3])\n",
    "print(os.environ[\"LANGCHAIN_TRACING_V2\"])\n",
    "# \"lsv\"と\"true\"が表示されれば、環境変数に設定できています"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"ユーザーが入力した料理のレシピを考えてください。\"),\n",
    "        (\"human\", \"{dish}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = init_chat_model(\n",
    "    model=\"gpt-5-nano\",\n",
    "    model_provider=\"openai\",\n",
    "    reasoning_effort=\"minimal\",\n",
    ")\n",
    "\n",
    "prompt_value = prompt.invoke({\"dish\": \"カレー\"})\n",
    "ai_message = model.invoke(prompt_value)\n",
    "print(ai_message.content)\n",
    "\n",
    "# ここでLangSmithのトレースを確認してください"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG (検索拡張生成) の基礎\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain での RAG の実装をステップバイステップで動かそう\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T12:06:08.360254Z",
     "iopub.status.busy": "2025-03-04T12:06:08.360058Z",
     "iopub.status.idle": "2025-03-04T12:06:08.863719Z",
     "shell.execute_reply": "2025-03-04T12:06:08.863362Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "\n",
    "loader = DirectoryLoader(\n",
    "    path=\"../tmp/langchain-docs/src/langsmith/\",\n",
    "    glob=\"**/*.mdx\",\n",
    "    loader_cls=TextLoader,\n",
    ")\n",
    "\n",
    "raw_docs = loader.load()\n",
    "print(len(raw_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T12:06:08.865062Z",
     "iopub.status.busy": "2025-03-04T12:06:08.864948Z",
     "iopub.status.idle": "2025-03-04T12:06:08.909769Z",
     "shell.execute_reply": "2025-03-04T12:06:08.909504Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "\n",
    "docs = text_splitter.split_documents(raw_docs)\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T12:06:08.910994Z",
     "iopub.status.busy": "2025-03-04T12:06:08.910877Z",
     "iopub.status.idle": "2025-03-04T12:06:08.922890Z",
     "shell.execute_reply": "2025-03-04T12:06:08.922485Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings import init_embeddings\n",
    "\n",
    "embeddings = init_embeddings(model=\"text-embedding-3-small\", provider=\"openai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T12:06:08.924092Z",
     "iopub.status.busy": "2025-03-04T12:06:08.923983Z",
     "iopub.status.idle": "2025-03-04T12:06:09.663993Z",
     "shell.execute_reply": "2025-03-04T12:06:09.663479Z"
    }
   },
   "outputs": [],
   "source": [
    "query = \"LangSmithのトレース機能について教えて\"\n",
    "\n",
    "vector = embeddings.embed_query(query)\n",
    "print(len(vector))\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T12:06:09.665905Z",
     "iopub.status.busy": "2025-03-04T12:06:09.665748Z",
     "iopub.status.idle": "2025-03-04T12:06:20.532208Z",
     "shell.execute_reply": "2025-03-04T12:06:20.530097Z"
    }
   },
   "outputs": [],
   "source": [
    "# 注意:\n",
    "# このセルの処理は、大勢が同時に実行するとレートリミットのエラーになる可能性があります\n",
    "# もしもエラーになった場合は、少し時間をおいてもう一度実行してください\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "vector_store = Chroma(embedding_function=embeddings)\n",
    "vector_store.reset_collection()\n",
    "vector_store.add_documents(docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T12:06:20.539733Z",
     "iopub.status.busy": "2025-03-04T12:06:20.539546Z",
     "iopub.status.idle": "2025-03-04T12:06:20.543979Z",
     "shell.execute_reply": "2025-03-04T12:06:20.543555Z"
    }
   },
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T12:06:20.545072Z",
     "iopub.status.busy": "2025-03-04T12:06:20.544965Z",
     "iopub.status.idle": "2025-03-04T12:06:20.812136Z",
     "shell.execute_reply": "2025-03-04T12:06:20.811266Z"
    }
   },
   "outputs": [],
   "source": [
    "query = \"LangSmithのトレース機能について教えて\"\n",
    "\n",
    "context_docs = retriever.invoke(query)\n",
    "print(f\"len = {len(context_docs)}\")\n",
    "\n",
    "for i, doc in enumerate(context_docs):\n",
    "    print(\n",
    "        \"-\" * 10 + f\" {i + 1}/{len(context_docs)}: {doc.metadata['source']} \" + \"-\" * 10\n",
    "    )\n",
    "    print(doc.page_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain を使った RAG の実装\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T12:06:20.819781Z",
     "iopub.status.busy": "2025-03-04T12:06:20.819465Z",
     "iopub.status.idle": "2025-03-04T12:06:20.840209Z",
     "shell.execute_reply": "2025-03-04T12:06:20.839637Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template('''\n",
    "以下の文脈だけを踏まえて質問に回答してください。\n",
    "\n",
    "文脈: \"\"\"\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "質問: {question}\n",
    "''')\n",
    "\n",
    "model = init_chat_model(\n",
    "    model=\"gpt-5-nano\",\n",
    "    model_provider=\"openai\",\n",
    "    reasoning_effort=\"minimal\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import traceable\n",
    "\n",
    "\n",
    "@traceable\n",
    "def invoke_rag(query: str) -> str:\n",
    "    documents = retriever.invoke(query)\n",
    "    prompt_value = prompt.invoke({\"question\": query, \"context\": documents})\n",
    "    ai_message = model.invoke(prompt_value)\n",
    "    return ai_message.content\n",
    "\n",
    "\n",
    "query = \"LangSmithのトレース機能について教えて\"\n",
    "output = invoke_rag(query)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
